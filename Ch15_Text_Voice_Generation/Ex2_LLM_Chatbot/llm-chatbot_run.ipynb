{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf791ca-fc60-4cf6-87a3-10242ca477fe",
   "metadata": {},
   "source": [
    "# Create an LLM-powered Chatbot using OpenVINO 精簡版\n",
    "by Jack OmniXRI, 2024/06/12\n",
    "\n",
    "為方便反覆測試各模型在不同硬體上執行效果，本程式參考 llm-chatbot.ipynb 進行修正。  \n",
    "\n",
    "這裡假設已執行過一次 llm-chatbot.ipynb 並下載好下列各檔案：  \n",
    "- llm_config.py\n",
    "- qwen2-1.5b-instruct 模型，並已轉換成 INT8 / INT4 二種格式\n",
    "\n",
    "註：如需其它模型及格式請先下載並轉換好才能運行本範例程式，以免運行時產生錯誤。 目前支援英文、簡中及日文所能對應的模型種類定義於 llm_config.py 中，下載前可先行查閱一下。  \n",
    "\n",
    "程式運行前建議進入選單【Edit】下， 執行【Clear Outputs of All Cells】清除所有輸出，再進入選單【Kernel】下， 執行【Restart Kernel...】清除所有記憶體。接下來可選擇【Restart Kernel and Run All Cells...】執行所有程式。\n",
    "  \n",
    "或者單步執行，依序選擇所需 Model_language(模型語言)、 Model(模型名稱)、 Device(運行裝置)、 Model to run(權重數值格式)，最後點擊 Running on local URL:  http://127.0.0.1:7860 網址，即可開始由Gradio互動網頁，開始對話。  \n",
    "\n",
    "註：選擇英文模型也可輸入中文對話。目前中文預訓練模型主要使用大陸大語言模型，回答內容為簡中輸出且較偏大陸用語，不習慣的可改用英文問答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93282b6-f1f1-4153-84af-31aac79c3ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_config import SUPPORTED_LLM_MODELS\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc663909-390a-41db-9718-b1b8b6206378",
   "metadata": {},
   "source": [
    "## 選擇模型語系  \n",
    "\n",
    "可選英文/簡中/日文。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02b34fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c45007ca04493c941362fea7529196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model Language:', options=('English', 'Chinese', 'Japanese'), value='English')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_languages = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "model_language = widgets.Dropdown(\n",
    "    options=model_languages,\n",
    "    value=model_languages[0],\n",
    "    description=\"Model Language:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83d305-bb4f-4cd9-ab23-548bbd245d99",
   "metadata": {},
   "source": [
    "## 選擇模型名稱  \n",
    "\n",
    "不同語系可選用模型數量不同，由 llm_config.py SUPPORTED_LLM_MODELS 定義。  \n",
    "\n",
    "- 英文： \"qwen2-0.5b-instruct\",  \"tiny-llama-1b-chat\", \"qwen2-1.5b-instruct\", \"gemma-2b-it\", \"phi-3-mini-instruct\", \"red-pajama-3b-chat\", \"qwen2-7b-instruct\", \"gemma-7b-it\", \"llama-2-chat-7b\", \"llama-3-8b-instruct\", \"mpt-7b-chat\", \"mistral-7b\", \"zephyr-7b-beta\", \"notus-7b-v1\", \"neural-chat-7b-v3-1\"  \n",
    "- 簡中： \"qwen1.5-0.5b-chat\", \"qwen2-1.5b-instruct\", \"qwen2-7b-instruct\", \"qwen1.5-7b-chat\", \"qwen-7b-chat\", \"chatglm3-6b\", \"baichuan2-7b-chat\", \"minicpm-2b-dpo\", \"internlm2-chat-1.8b\", \"qwen1.5-1.8b-chat\"  \n",
    "- 日文： \"youri-7b-chat\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d22fedb-d1f6-4306-b910-efac5b849c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a0f6f93b614b22a82dd51317b645ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('qwen1.5-0.5b-chat', 'qwen2-1.5b-instruct', 'qwen2-7b-instruct', 'qwen…"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ids = list(SUPPORTED_LLM_MODELS[model_language.value])\n",
    "\n",
    "model_id = widgets.Dropdown(\n",
    "    options=model_ids,\n",
    "    value=model_ids[0],\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "906022ec-96bf-41a9-9447-789d2e875250",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model qwen1.5-0.5b-chat\n"
     ]
    }
   ],
   "source": [
    "model_configuration = SUPPORTED_LLM_MODELS[model_language.value][model_id.value]\n",
    "print(f\"Selected model {model_id.value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "130a037a-7d98-4152-81ea-92ffb01da5a2",
   "metadata": {},
   "source": [
    "We can now save floating point and compressed model variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3652e0-5850-4edc-9aef-d120f772a781",
   "metadata": {},
   "source": [
    "## 設定模型讀取路徑  \n",
    "\n",
    "設定模型路徑並檢查指定的權重數值格式(FP16/INT8/INT4)檔案是否存在。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4ef9112",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "pt_model_id = model_configuration[\"model_id\"]\n",
    "pt_model_name = model_id.value.split(\"-\")[0]\n",
    "fp16_model_dir = Path(model_id.value) / \"FP16\"\n",
    "int8_model_dir = Path(model_id.value) / \"INT8_compressed_weights\"\n",
    "int4_model_dir = Path(model_id.value) / \"INT4_compressed_weights\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "671a17d4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's compare model size for different compression types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "281f1d07-998e-4e13-ba95-0264564ede82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "if fp16_weights.exists():\n",
    "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    if compressed_weights.exists() and fp16_weights.exists():\n",
    "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e94a6-901a-4058-94fa-114c0049ba17",
   "metadata": {},
   "source": [
    "## 選擇推論硬體  \n",
    "\n",
    "若有 NPU 會自動加入下拉式選單中，預設會有 CPU / GPU / AUTO 三種選項。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "837b4a3b-ccc3-4004-9577-2b2c7b802dea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7881e3049dd34c06bff10ae30e68183a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "support_devices = core.available_devices\n",
    "if \"NPU\" in support_devices:\n",
    "    support_devices.remove(\"NPU\")\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=support_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c53001e7-615f-4eb5-b831-4e2b2ff32826",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 指定權重數值  \n",
    "\n",
    "依已下載及轉換好的模型權重值類型加入下拉式選單中。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3536a1a7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fp16_model_dir\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m      7\u001b[0m     available_models\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m model_to_run \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mDropdown(\n\u001b[0;32m     10\u001b[0m     options\u001b[38;5;241m=\u001b[39mavailable_models,\n\u001b[1;32m---> 11\u001b[0m     value\u001b[38;5;241m=\u001b[39m\u001b[43mavailable_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     12\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel to run:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     disabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m model_to_run\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "available_models = []\n",
    "if int4_model_dir.exists():\n",
    "    available_models.append(\"INT4\")\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "if fp16_model_dir.exists():\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82d2d0-4c5e-4eed-bf26-3c215dae4e3e",
   "metadata": {},
   "source": [
    "## 建立 optimum.intel 所需參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a041101-7336-40fd-96c9-cd298015a0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "\n",
    "if model_to_run.value == \"INT4\":\n",
    "    model_dir = int4_model_dir\n",
    "elif model_to_run.value == \"INT8\":\n",
    "    model_dir = int8_model_dir\n",
    "else:\n",
    "    model_dir = fp16_model_dir\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "if \"GPU\" in device.value and \"qwen2-7b-instruct\" in model_id.value:\n",
    "    ov_config[\"GPU_ENABLE_SDPA_OPTIMIZATION\"] = \"NO\"\n",
    "\n",
    "# On a GPU device a model is executed in FP16 precision. For red-pajama-3b-chat model there known accuracy\n",
    "# issues caused by this, which we avoid by setting precision hint to \"f32\".\n",
    "if model_id.value == \"red-pajama-3b-chat\" and \"GPU\" in core.available_devices and device.value in [\"GPU\", \"AUTO\"]:\n",
    "    ov_config[\"INFERENCE_PRECISION_HINT\"] = \"f32\"\n",
    "\n",
    "model_name = model_configuration[\"model_id\"]\n",
    "tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "ov_model = OVModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device=device.value,\n",
    "    ov_config=ov_config,\n",
    "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29147846-7af7-43af-8144-1f3de0738113",
   "metadata": {},
   "source": [
    "## 取得詞元關鍵字參數並測試\n",
    "\n",
    "tokenizer keyword arguments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f6f7596-5677-4931-875b-aaabfa23cabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = ?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
    "test_string = \"2 + 2 =\"\n",
    "input_tokens = tok(test_string, return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "answer = ov_model.generate(**input_tokens, max_new_tokens=2)\n",
    "print(tok.batch_decode(answer, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd91dba-f64a-49c5-a86e-0cfaadbe8621",
   "metadata": {},
   "source": [
    "## 運行對話機器人並產生Gradio型式互動式界面  \n",
    "\n",
    "順利執行後會在 http://127.0.0.1:7860 產生一個新互動式界面，可直接輸入對話及顯示答覆。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01f8f7f8-072e-45dc-b7c9-18d8c3c47754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from threading import Event, Thread\n",
    "from uuid import uuid4\n",
    "from typing import List, Tuple\n",
    "import gradio as gr\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer,\n",
    ")\n",
    "\n",
    "\n",
    "model_name = model_configuration[\"model_id\"]\n",
    "start_message = model_configuration[\"start_message\"]\n",
    "history_template = model_configuration.get(\"history_template\")\n",
    "current_message_template = model_configuration.get(\"current_message_template\")\n",
    "stop_tokens = model_configuration.get(\"stop_tokens\")\n",
    "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
    "\n",
    "chinese_examples = [\n",
    "    [\"你好!\"],\n",
    "    [\"你是谁?\"],\n",
    "    [\"请介绍一下上海\"],\n",
    "    [\"请介绍一下英特尔公司\"],\n",
    "    [\"晚上睡不着怎么办？\"],\n",
    "    [\"给我讲一个年轻人奋斗创业最终取得成功的故事。\"],\n",
    "    [\"给这个故事起一个标题。\"],\n",
    "]\n",
    "\n",
    "english_examples = [\n",
    "    [\"Hello there! How are you doing?\"],\n",
    "    [\"What is OpenVINO?\"],\n",
    "    [\"Who are you?\"],\n",
    "    [\"Can you explain to me briefly what is Python programming language?\"],\n",
    "    [\"Explain the plot of Cinderella in a sentence.\"],\n",
    "    [\"What are some common mistakes to avoid when writing code?\"],\n",
    "    [\"Write a 100-word blog post on “Benefits of Artificial Intelligence and OpenVINO“\"],\n",
    "]\n",
    "\n",
    "japanese_examples = [\n",
    "    [\"こんにちは！調子はどうですか?\"],\n",
    "    [\"OpenVINOとは何ですか?\"],\n",
    "    [\"あなたは誰ですか?\"],\n",
    "    [\"Pythonプログラミング言語とは何か簡単に説明してもらえますか?\"],\n",
    "    [\"シンデレラのあらすじを一文で説明してください。\"],\n",
    "    [\"コードを書くときに避けるべきよくある間違いは何ですか?\"],\n",
    "    [\"人工知能と「OpenVINOの利点」について100語程度のブログ記事を書いてください。\"],\n",
    "]\n",
    "\n",
    "examples = chinese_examples if (model_language.value == \"Chinese\") else japanese_examples if (model_language.value == \"Japanese\") else english_examples\n",
    "\n",
    "max_new_tokens = 256\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "if stop_tokens is not None:\n",
    "    if isinstance(stop_tokens[0], str):\n",
    "        stop_tokens = tok.convert_tokens_to_ids(stop_tokens)\n",
    "\n",
    "    stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "\n",
    "\n",
    "def default_partial_text_processor(partial_text: str, new_text: str):\n",
    "    \"\"\"\n",
    "    helper for updating partially generated answer, used by default\n",
    "\n",
    "    Params:\n",
    "      partial_text: text buffer for storing previosly generated text\n",
    "      new_text: text update for the current step\n",
    "    Returns:\n",
    "      updated text string\n",
    "\n",
    "    \"\"\"\n",
    "    partial_text += new_text\n",
    "    return partial_text\n",
    "\n",
    "\n",
    "text_processor = model_configuration.get(\"partial_text_processor\", default_partial_text_processor)\n",
    "\n",
    "\n",
    "def convert_history_to_token(history: List[Tuple[str, str]]):\n",
    "    \"\"\"\n",
    "    function for conversion history stored as list pairs of user and assistant messages to tokens according to model expected conversation template\n",
    "    Params:\n",
    "      history: dialogue history\n",
    "    Returns:\n",
    "      history in token format\n",
    "    \"\"\"\n",
    "    if pt_model_name == \"baichuan2\":\n",
    "        system_tokens = tok.encode(start_message)\n",
    "        history_tokens = []\n",
    "        for old_query, response in history[:-1]:\n",
    "            round_tokens = []\n",
    "            round_tokens.append(195)\n",
    "            round_tokens.extend(tok.encode(old_query))\n",
    "            round_tokens.append(196)\n",
    "            round_tokens.extend(tok.encode(response))\n",
    "            history_tokens = round_tokens + history_tokens\n",
    "        input_tokens = system_tokens + history_tokens\n",
    "        input_tokens.append(195)\n",
    "        input_tokens.extend(tok.encode(history[-1][0]))\n",
    "        input_tokens.append(196)\n",
    "        input_token = torch.LongTensor([input_tokens])\n",
    "    elif history_template is None:\n",
    "        messages = [{\"role\": \"system\", \"content\": start_message}]\n",
    "        for idx, (user_msg, model_msg) in enumerate(history):\n",
    "            if idx == len(history) - 1 and not model_msg:\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "                break\n",
    "            if user_msg:\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            if model_msg:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "\n",
    "        input_token = tok.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\")\n",
    "    else:\n",
    "        text = start_message + \"\".join(\n",
    "            [\"\".join([history_template.format(num=round, user=item[0], assistant=item[1])]) for round, item in enumerate(history[:-1])]\n",
    "        )\n",
    "        text += \"\".join(\n",
    "            [\n",
    "                \"\".join(\n",
    "                    [\n",
    "                        current_message_template.format(\n",
    "                            num=len(history) + 1,\n",
    "                            user=history[-1][0],\n",
    "                            assistant=history[-1][1],\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        input_token = tok(text, return_tensors=\"pt\", **tokenizer_kwargs).input_ids\n",
    "    return input_token\n",
    "\n",
    "\n",
    "def user(message, history):\n",
    "    \"\"\"\n",
    "    callback function for updating user messages in interface on submit button click\n",
    "\n",
    "    Params:\n",
    "      message: current message\n",
    "      history: conversation history\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    # Append the user's message to the conversation history\n",
    "    return \"\", history + [[message, \"\"]]\n",
    "\n",
    "\n",
    "def bot(history, temperature, top_p, top_k, repetition_penalty, conversation_id):\n",
    "    \"\"\"\n",
    "    callback function for running chatbot on submit button click\n",
    "\n",
    "    Params:\n",
    "      history: conversation history\n",
    "      temperature:  parameter for control the level of creativity in AI-generated text.\n",
    "                    By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.\n",
    "      top_p: parameter for control the range of tokens considered by the AI model based on their cumulative probability.\n",
    "      top_k: parameter for control the range of tokens considered by the AI model based on their cumulative probability, selecting number of tokens with highest probability.\n",
    "      repetition_penalty: parameter for penalizing tokens based on how frequently they occur in the text.\n",
    "      conversation_id: unique conversation identifier.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the input message string for the model by concatenating the current system message and conversation history\n",
    "    # Tokenize the messages string\n",
    "    input_ids = convert_history_to_token(history)\n",
    "    if input_ids.shape[1] > 2000:\n",
    "        history = [history[-1]]\n",
    "        input_ids = convert_history_to_token(history)\n",
    "    streamer = TextIteratorStreamer(tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=temperature > 0.0,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    if stop_tokens is not None:\n",
    "        generate_kwargs[\"stopping_criteria\"] = StoppingCriteriaList(stop_tokens)\n",
    "\n",
    "    stream_complete = Event()\n",
    "\n",
    "    def generate_and_signal_complete():\n",
    "        \"\"\"\n",
    "        genration function for single thread\n",
    "        \"\"\"\n",
    "        global start_time\n",
    "        ov_model.generate(**generate_kwargs)\n",
    "        stream_complete.set()\n",
    "\n",
    "    t1 = Thread(target=generate_and_signal_complete)\n",
    "    t1.start()\n",
    "\n",
    "    # Initialize an empty string to store the generated text\n",
    "    partial_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_text = text_processor(partial_text, new_text)\n",
    "        history[-1][1] = partial_text\n",
    "        yield history\n",
    "\n",
    "\n",
    "def request_cancel():\n",
    "    ov_model.request.cancel()\n",
    "\n",
    "\n",
    "def get_uuid():\n",
    "    \"\"\"\n",
    "    universal unique identifier for thread\n",
    "    \"\"\"\n",
    "    return str(uuid4())\n",
    "\n",
    "\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\".disclaimer {font-variant-caps: all-small-caps;}\",\n",
    ") as demo:\n",
    "    conversation_id = gr.State(get_uuid)\n",
    "    gr.Markdown(f\"\"\"<h1><center>OpenVINO {model_id.value} Chatbot</center></h1>\"\"\")\n",
    "    chatbot = gr.Chatbot(height=500)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Chat Message Box\",\n",
    "                placeholder=\"Chat Message Box\",\n",
    "                show_label=False,\n",
    "                container=False,\n",
    "            )\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                submit = gr.Button(\"Submit\")\n",
    "                stop = gr.Button(\"Stop\")\n",
    "                clear = gr.Button(\"Clear\")\n",
    "    with gr.Row():\n",
    "        with gr.Accordion(\"Advanced Options:\", open=False):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        temperature = gr.Slider(\n",
    "                            label=\"Temperature\",\n",
    "                            value=0.1,\n",
    "                            minimum=0.0,\n",
    "                            maximum=1.0,\n",
    "                            step=0.1,\n",
    "                            interactive=True,\n",
    "                            info=\"Higher values produce more diverse outputs\",\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        top_p = gr.Slider(\n",
    "                            label=\"Top-p (nucleus sampling)\",\n",
    "                            value=1.0,\n",
    "                            minimum=0.0,\n",
    "                            maximum=1,\n",
    "                            step=0.01,\n",
    "                            interactive=True,\n",
    "                            info=(\n",
    "                                \"Sample from the smallest possible set of tokens whose cumulative probability \"\n",
    "                                \"exceeds top_p. Set to 1 to disable and sample from all tokens.\"\n",
    "                            ),\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        top_k = gr.Slider(\n",
    "                            label=\"Top-k\",\n",
    "                            value=50,\n",
    "                            minimum=0.0,\n",
    "                            maximum=200,\n",
    "                            step=1,\n",
    "                            interactive=True,\n",
    "                            info=\"Sample from a shortlist of top-k tokens — 0 to disable and sample from all tokens.\",\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        repetition_penalty = gr.Slider(\n",
    "                            label=\"Repetition Penalty\",\n",
    "                            value=1.1,\n",
    "                            minimum=1.0,\n",
    "                            maximum=2.0,\n",
    "                            step=0.1,\n",
    "                            interactive=True,\n",
    "                            info=\"Penalize repetition — 1.0 to disable.\",\n",
    "                        )\n",
    "    gr.Examples(examples, inputs=msg, label=\"Click on any example and press the 'Submit' button\")\n",
    "\n",
    "    submit_event = msg.submit(\n",
    "        fn=user,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=bot,\n",
    "        inputs=[\n",
    "            chatbot,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            repetition_penalty,\n",
    "            conversation_id,\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    submit_click_event = submit.click(\n",
    "        fn=user,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=bot,\n",
    "        inputs=[\n",
    "            chatbot,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            repetition_penalty,\n",
    "            conversation_id,\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    stop.click(\n",
    "        fn=request_cancel,\n",
    "        inputs=None,\n",
    "        outputs=None,\n",
    "        cancels=[submit_event, submit_click_event],\n",
    "        queue=False,\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "#  demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# if you have any issue to launch on your platform, you can pass share=True to launch method:\n",
    "# demo.launch(share=True)\n",
    "# it creates a publicly shareable link for the interface. Read more in the docs: https://gradio.app/docs/\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b837f9e-4152-4a5c-880a-ed874aa64a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# please uncomment and run this cell for stopping gradio interface\n",
    "# demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/29454499/255799218-611e7189-8979-4ef5-8a80-5a75e0136b50.png",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation",
     "Conversational"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
